{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://academics.sheridancollege.ca/programs/computer-engineering-technology\"><img src=\"https://www.sheridancollege.ca/-/media/project/sheridan/shared/images/logos/sheridan-logo-header.svg\" width=\"300\" align=\"center\"></a>\n",
    "\n",
    "\n",
    "<h1><center>AI Application (ENGI51071) - Lab 7 </center></h1>\n",
    "<h2><center>Student Name: Confidence Ahuekwe | Student Number: 991714953 | Date: 28th March, 2024.</center></h2> \n",
    "\n",
    "\n",
    "<h3>Lab Overview</h3>\n",
    "This is part of the lab series for AI Application (ENGI51071).In this lab, we are going to explore tools and techniques that are related to a conversational agent.\n",
    "\n",
    "\n",
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href='#Library_Installation'>Library Installation</a></li>\n",
    "        <li><a href='#Import_Needed_Packages'>Import Needed Packages</a></li>\n",
    "        <li><a href='#Basic_NLP_Techniques'>Basic NLP Techniques</a></li>\n",
    "        <li><a href='#NLP_ML_Pipeline'>NLP-ML Pipeline </a></li>   \n",
    "        <li><a href='#Discussion_and_Questions'>Discussion and Questions</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Library_Installation'></a>\n",
    "<h2> 1. Library Installation </h2>\n",
    "<h3> For this lab, the following new packages need to be installed </h3><br>\n",
    "NLTK: Natural Language Toolkit for Python. <br>\n",
    "Your may use Anaconda's package managing system to help with the installation. Use the following steps: \n",
    "    <ol>\n",
    "        <li>In the home screen, choose your working virtual environment</li>\n",
    "        <li>Go to Environments, list \"not installed\"</li>\n",
    "        <li>Search for package \"nltk\"</li>\n",
    "        <li>Select both \"nltk\" and \"nltk_data\", click Apply on the lower right corner to install</li>   \n",
    "    </ol>\n",
    "Please note that the installation takes a while and please ensure this step is completed before moving onto part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Import_Needed_Packages'></a>\n",
    "<h2> 2. Import Needed packages </h2>\n",
    "<h3> For this lab, the following packages need to be imported </h3><br>\n",
    "numpy: Python library for working with arrays<br>\n",
    "pandas: Python Data Analysis Library to work with dataframes<br>\n",
    "sklearn: scikit-learn, a commonly used machine learning library. <br>\n",
    "NLTK: Natural Language Toolkit for Python. <br>\n",
    "re: a commonly used regex resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\vision2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Basic_NLK_Techniques'></a>\n",
    "<h2> 3. basic NLK Techniques </h2>\n",
    "<h3> In this section, we are going to follow a simple NLP pipeline to practice some simple NLP techniques </h3><br>\n",
    "Firstly, read in a semi-structured data set.  \n",
    "In this lab, we are going to use the UCI SMS Spam Collection dataset provided by The University of California, Irvine  repository. You may find more information about this dataset from the link below: <br>\n",
    "\n",
    "<h4>UCI SMS Spam Collection Dataset Repository link: <a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#\">click here</a></h4><br>\n",
    "In the cell below, read in the raw data file \"SMSSpamCollection\".  <br>\n",
    "Please note, this file format raw data is stored in a way that is different from our previous lab files and some modifications are done to accommodate such differences. \n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples: <br>\n",
    "\n",
    "ham   What you doing? how are you? <br>\n",
    "ham   Ok lar... Joking wif u oni...<br>\n",
    "ham   dun say so early hor... U c already then say...<br>\n",
    "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*<br>\n",
    "ham   Siva is in hostel aha:-.<br>\n",
    "ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.<br>\n",
    "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop<br>\n",
    "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B<br>\n",
    "spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU<br>\n",
    "<br>\n",
    "Note 1: messages are not chronologically sorted.<br>\n",
    "Note 2: messages are separated by tabs '\\t'.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.1 Data import </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step1. read in the raw data and display the first 500 characters of the raw data file \n",
    "f = open('SMSSpamCollection','r')\n",
    "f.read(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(0.5') Question 1__:  Based on the information provided, is the dataset structured, semi-structured or unstructured? Why? <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "__Ans:__ Based on the provided information, it is deduced that the dataset is semi-structured. Here’s why:\n",
    "\n",
    "The data consists of text messages marked as either “ham” (non-spam) or “spam.”\n",
    "The messages are in English and vary in style (formal and informal).\n",
    "There is no clear format or organization, making it challenging to analyze or process without further transformation.\n",
    "In summary, the lack of structure or predefined schema classifies this dataset as semi-structured. While it can be converted into a structured format, it isn’t inherently structured like a table or database. <br>\n",
    "\n",
    "__(0.5') Question 2__:  Does this raw data file have any header information? <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "__Ans:__ The raw data file does not contain any header information. Each line in the file consists of two columns: one with the label (either “ham” or “spam”) and the other with the raw text of the SMS messages. There is no additional metadata or header present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any raw data files can be opened using file open method.  However, let's take advantage of the higher level file interfacing features of Python to read in the data in its proper format and reduce unnecessary steps. <br>\n",
    "In the cell below, we are going to read in the same file but using Pandas' csv_read to directly read the raw data into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           SMS_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None)\n",
    "sms_df.columns = ['label', 'SMS_text']\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           SMS_text\n",
       "5563  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5564   ham               Will ü b going to esplanade fr home?\n",
       "5565   ham  Pity, * was in mood for that. So...any other s...\n",
       "5566   ham  The guy did some bitching but I acted like i'd...\n",
       "5567   ham                         Rofl. Its true to its name"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 Data overview </h3><br>\n",
    "A quick observation of the raw data is that the SMS text body contains a string of characters that computer is not able to comprehend. In order for it to work properly,  we are going to take a look at the nature of the data more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5568 entries in this dataset.\n"
     ]
    }
   ],
   "source": [
    "# in this cell find the number of entries of the dataset. \n",
    "print(\"There are {} entries in this dataset.\".format(len(sms_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4822 ham entries in this dataset\n",
      "There are 746 spam entries in this dataset\n"
     ]
    }
   ],
   "source": [
    "# in this cell find the number of HAM entries and the number of SPAM entries. \n",
    "print(\"There are {} ham entries in this dataset\".format(len(sms_df[sms_df['label']=='ham'])))\n",
    "print(\"There are {} spam entries in this dataset\".format(len(sms_df[sms_df['label']=='spam'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5568</td>\n",
       "      <td>5568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4822</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                SMS_text\n",
       "count   5568                    5568\n",
       "unique     2                    5165\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4822                      30"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An easier way to do this is to use the build-in describefunction of Pandas.\n",
    "sms_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 Regular Expression and Libray re </h3><br>\n",
    "In Python, string manipulation is facilitated by the the proper use of Python's Regular Expression operators (library re).   <br> \n",
    "A detailed howto document on Regular Expression operators can be find in the link <a href=\"https://docs.python.org/3/howto/regex.html#regex-howto\">here.</a><br>\n",
    "\n",
    "Simply put, regex is a search term that can be used when scanning a string. \n",
    "Most importantly, the following regex are commonly used in tokenization. You may use square brackets [ ] to specify a set of characters that you wish to match. \n",
    "For example, [a-d] will match any characters a, b, or c. [a-z][A-Z] will match any lower case letter followed by an upper case letters. \n",
    "\n",
    "\\d: Matches any decimal digit; this is equivalent to the class [0-9].<br>\n",
    "\\D: Matches any non-digit character; this is equivalent to the class [^0-9].<br>\n",
    "\\s: Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].<br>\n",
    "\\S: Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].<br>\n",
    "\\w: Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].<br>\n",
    "\\W: Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interesting',\n",
       " 'topic.',\n",
       " 'However,',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'other',\n",
       " 'interesting',\n",
       " 'topics.',\n",
       " '']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # import regex resource\n",
    "# Experimenting tokenization\n",
    "\n",
    "test_string = \"Artificial intelligence    is an interesting topic.    However, there are many other interesting topics.  \"\n",
    "re.split('\\s+', test_string) # split at every location where whitespace is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1') Question 3__:  Is this tokenization sucessful? Why? <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "__Ans:__ The tokenization is considered successful.\n",
    "\n",
    "Successful Aspects:\n",
    "The code snippet uses the re.split() function to split the test string based on whitespace characters.\n",
    "It successfully separates the test string into individual tokens (words) using spaces as delimiters.\n",
    "For example, “artificial” and “intelligence” are correctly identified as separate tokens.\n",
    "Limitations:\n",
    "The tokenization process does not handle special characters or punctuation well.\n",
    "Overall Assessment:\n",
    "While the basic tokenization using whitespace is a good starting point, more sophisticated techniques (such as handling punctuation and contractions) are needed for better results. Depending on the specific use case, additional preprocessing steps may be required to improve the quality of tokenization.\n",
    "In summary, the tokenization is considered successful, but it could be enhanced to handle punctuation and special cases more effectively. <br>\n",
    "\n",
    "\n",
    "__(1') Question 4__:  Can you change the search term regex to improve the tokenization result? <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "__Ans:__ Yes, improving tokenization using regular expressions is essential for accurate text processing. \n",
    "A good way to enhance the tokenization result is the Precedence-Based Tokenization:\n",
    "The Precedence-Based Regex Tokenizer is a powerful approach that balances simplicity and performance.\n",
    "Instead of sequentially checking which TokenDefinitions match, we gather all matches into a list and assign a precedence value to each match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interesting',\n",
       " 'topic',\n",
       " 'However',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'other',\n",
       " 'interesting',\n",
       " 'topics',\n",
       " '']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code here\n",
    "\n",
    "# Example text\n",
    "test_string2 = \"Artificial intelligence is an interesting topic. However, there are many other interesting topics.\"\n",
    "\n",
    "# Tokenize using regex\n",
    "re.split(r'\\W+', test_string2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of spliting where a certain regex is found, you may only report back the scanning result or replace the found hits.  Four methods are commonly used in re class. \n",
    "\n",
    "| Name         | Purpose      | \n",
    "| :------------- |:-----------------------------------------------------------------:| \n",
    "| findall        |  Returns a list containing all matches | \n",
    "| search        |  Returns a Match object if there is a match anywhere in the string          | \n",
    "| split        |  Returns a list where the string has been split at each match     | it on screen    |\n",
    "| sub        |  Replaces one or many matches with a string | save your answer to a variable and then print it on screen| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ar', 'Ho']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]', test_string) # this line will find one upper case followed by one lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interesting',\n",
       " 'topic',\n",
       " 'However',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'other',\n",
       " 'interesting',\n",
       " 'topics']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', test_string) # this line will return the list of words that is one or more than 1 character long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.4 Punctuation removal </h3><br>\n",
    "Removing punction is essentially scan the string and remove match symbols.  The common punctuation list is available in string.punctuation. To remove, simply chech each character against the punctuation list and decide to keep or remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "      <th>text_nopunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           SMS_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                        text_nopunct  \n",
       "0  ive been searching for the right words to than...  \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "2  nah i dont think he goes to usf he lives aroun...  \n",
       "3  even my brother is not like to speak with me t...  \n",
       "4                  i have a date on sunday with will  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "def punct_remove(raw_text):\n",
    "    text_nopunct = \"\".join([char for char in raw_text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "#create a new column to store the tokenized SMS text\n",
    "sms_df['text_nopunct'] = sms_df['SMS_text'].apply(lambda x: punct_remove(x.lower()))\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.5 Tokenization </h3><br>\n",
    "Tokenization is to split a string of characters into pieces, each piece is called a token.  A successful tokenization is mainly determined by how effective the split algorithm is. <br>\n",
    "Now based on the regex rules that we have learned, tokenize each SMS_text body into individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "      <th>text_nopunct</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           SMS_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                        text_nopunct  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [ive, been, searching, for, the, right, words,...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...  \n",
       "3  [even, my, brother, is, not, like, to, speak, ...  \n",
       "4         [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function that tokenizes an input text string\n",
    "def tokenize(raw_text):\n",
    "    tokens = re.split('\\W+', raw_text)\n",
    "    return tokens\n",
    "\n",
    "#create a new column to store the tokenized SMS text\n",
    "sms_df['text_tokens'] = sms_df['text_nopunct'].apply(lambda x: tokenize(x.lower()))\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.6 Stop word removal </h3><br>\n",
    "Similar to punctuations, a list of stop words is available in nltk library.  Simply import this list and compare each token against this list.  Keep the word only if it is not on the stop word list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "      <th>text_nopunct</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           SMS_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                        text_nopunct  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                         text_nostop  \n",
       "0  [ive, searching, right, words, thank, breather...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...  \n",
       "3  [even, brother, like, speak, treat, like, aids...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def stopword_remove(tokens):\n",
    "    text_nostop = [word for word in tokens if word not in stopwords]\n",
    "    return text_nostop\n",
    "\n",
    "#create a new column to store the tokenized SMS text\n",
    "sms_df['text_nostop'] = sms_df['text_tokens'].apply(stopword_remove)\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.7 Stemming and lemmatization </h3><br>\n",
    "Once the raw text has been tokenized, punctuations and stop words removed, a supplementary step is to stem or lemmatize the word list to group similar words or different forms of the same words together. <br>\n",
    "\n",
    "First, we compare the commonly used WordNet lemmatizer and Porter stemmer: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe\n",
      "university\n",
      "universal\n",
      "univers\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "print(wn.lemmatize('universe'))\n",
    "print(wn.lemmatize('university'))\n",
    "print(wn.lemmatize('universal'))\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "print(ps.stem('universe'))\n",
    "print(ps.stem('university'))\n",
    "print(ps.stem('universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1') Question 5__:  What type of error does stemmer exhibit here? How to fix this problem? <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "__Ans:__ The error message from the stemmer is a ResourceNotFoundError related to the WordNet resource. Let’s break down the issue and discuss how to fix it:\n",
    "\n",
    "Error Explanation:\n",
    "The error message indicates that the WordNet resource is not found.\n",
    "WordNet is a lexical database used for lemmatization and other natural language processing tasks.\n",
    "The code is trying to initialize a WordNet Lemmatizer using nltk.WordNetLemmatizer().\n",
    "However, the required WordNet data files are missing.\n",
    "Solution: To resolve this issue, we need to download the WordNet data files following these steps: \n",
    "\n",
    "Open a Python shell or a script, Import the nltk library: import nltk<br>\n",
    "Download the WordNet data using: nltk.download('wordnet'),\n",
    "This command will download the necessary files for WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here’s the updated code:\n",
    "# Download WordNet data\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe\n",
      "university\n",
      "universal\n",
      "univers\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "# Initialize WordNet Lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words\n",
    "print(wn.lemmatize('universe'))\n",
    "print(wn.lemmatize('university'))\n",
    "print(wn.lemmatize('universal'))\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# Stem words\n",
    "print(ps.stem('universe'))\n",
    "print(ps.stem('university'))\n",
    "print(ps.stem('universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to use the WordNet lemmatizer on the tokenized cleaned text_body. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS_text</th>\n",
       "      <th>text_nopunct</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           SMS_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                        text_nopunct  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                         text_nostop  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  [ive, searching, right, word, thank, breather,...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, go, usf, life, around, though]  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(tokens):\n",
    "    text_lemmatized = [wn.lemmatize(word) for word in tokens]\n",
    "    return text_lemmatized\n",
    "\n",
    "#create a new column to store the tokenized SMS text\n",
    "sms_df['text_lemmatized'] = sms_df['text_nostop'].apply(lambda x: lemmatize(x))\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1') Task 1:__ Now you are familiarized with all the basic NLP steps, please combine punctuation removel, tokenization and lemmatization to create a function called text_clean. Please record your answer on lab7 on SLATE quiz tool. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # test your code here \n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def text_clean(raw_text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text by performing the following steps:\n",
    "    1. Remove punctuation\n",
    "    2. Tokenize the text\n",
    "    3. Remove stopwords\n",
    "    4. Lemmatize the tokens\n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): Input text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text_nopunct = raw_text.translate(translator)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = re.split(r\"\\W+\", text_nopunct.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Join the lemmatized tokens back into a cleaned text\n",
    "    cleaned_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is an example sentence with punctuation, stopwords, and various forms of words.\n",
      "Cleaned text: example sentence punctuation stopwords various form word\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "input_text = \"This is an example sentence with punctuation, stopwords, and various forms of words.\"\n",
    "cleaned_text = text_clean(input_text)\n",
    "print(f\"Original text: {input_text}\")\n",
    "print(f\"Cleaned text: {cleaned_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.8 Vectorization </h3><br>\n",
    "Once the raw text has been tokenized, punctuations and stop words removed, a supplementary step is to stem or lemmatize the word list to group similar words or different forms of the same words together. <br>\n",
    "With all the data processed and clean.  We are ready to convert the clearned text into feature vectors. \n",
    "As mentioned in lecture, three types of vectorizers are introduced.  For simplicity, we are going to implement the most widely used vectorizer TF-IDF vectorizer. It is located in sklearn.feature_extraction package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      6\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer(analyzer\u001b[38;5;241m=\u001b[39mdata_clean)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=data_clean)\n",
    "X_tfidf = tfidf_vect.fit_transform(sms_df['SMS_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NLP_ML_Pipeline'></a>\n",
    "<h2>4. NLP-ML Pipeline </h2>\n",
    "\n",
    "With all the data processed, cleaned and vectorized,  we are ready to apply machine learning algorithms to this data set. This time, we are going to use Random Forest classifier to create the prediction model.  Random forest essentially creates a group of decision trees and makes the final decision through majority voting.  Hence its name \"forest\". <br>\n",
    "__(1.5') Task 2__.  In the cell below to create a random forest classifier with the name \"rf_model\".  You may change your input parameters to ensure better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1.5' Please test your code here \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1.5' Please test your code here \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Here, we will report the accuracy of the classifier in a cross-validation step. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(rf_model, X_features, sms_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Here, we will report the accuracy of the classifier in a cross-validation step. \n",
    "scores = cross_val_score(rf_model, X_features, sms_df['label'], cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print('The reported accuracy using K-fold cross validation is: %.2f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1.5') Question 6:__ In this case, do we have a training and test dataset split?  If yes, which line does the split; if no, why is it no longer needed?  <br>\n",
    "Please record your answer on lab7 on SLATE quiz tool. <br>\n",
    "\n",
    "__Ans:__ The provided code snippet does not explicitly split the dataset into training and test sets. Instead, it performs k-fold cross-validation to estimate the model’s accuracy.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "Cross-Validation:\n",
    "The dataset is divided into k subsets (folds).\n",
    "The model is trained k times, each time using k-1 folds for training and the remaining fold for validation.\n",
    "The final accuracy is the average of the accuracies obtained in each fold.\n",
    "Details:\n",
    "The rf_model is trained and evaluated using 10-fold cross-validation.\n",
    "The X_tfidf features are used.\n",
    "The target variable is sms_df['label'] (presumably the class labels).\n",
    "No Explicit Train-Test Split:\n",
    "There is no explicit train-test split; the entire dataset is used for cross-validation.\n",
    "Cross-validation provides a more robust estimate of model performance by evaluating it on different subsets of the data.\n",
    "Final Evaluation:\n",
    "If we need a separate test set for final evaluation, we would split the data explicitly using train-test split.\n",
    "However, in this snippet, cross-validation suffices for reporting the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Discussion_and_Questions'></a>\n",
    "<h2> 5. Discussion and Questions </h2>\n",
    "<h3> Please use your own words to answer the following questions </h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1') Question 7:__ Propose at least two different ways that are different from the in-class discussion to improve your feature vectors.  <br>\n",
    "__Ans:__ Word Embeddings for Text Features:\n",
    "Word embeddings (such as Word2Vec, GloVe, or FastText) capture semantic relationships between words by mapping them to dense vector representations in a continuous space.\n",
    "Instead of using raw text features (e.g., bag-of-words or TF-IDF), consider embedding each word into a fixed-size vector.\n",
    "These embeddings can be pre-trained on large corpora or fine-tuned on domain-specific data.\n",
    "Benefits:\n",
    "Captures contextual information and word semantics.\n",
    "Reduces dimensionality compared to one-hot encoding.\n",
    "Encodes similarity between words.\n",
    "Example: In a sentiment analysis task, use pre-trained word embeddings to represent text features, enhancing the model’s understanding of sentiment-related words.\n",
    "Interaction Features:\n",
    "Interaction features are derived from the combination of existing features.\n",
    "They capture relationships between pairs or groups of features.\n",
    "Examples of interaction features:\n",
    "Product Features: Multiply two continuous features (e.g., age × income) to create a new feature representing their joint effect.\n",
    "Categorical Combinations: Combine two categorical features (e.g., gender and occupation) to create a new feature (e.g., “male_engineer”).\n",
    "Feature Ratios: Divide one feature by another (e.g., debt-to-income ratio).\n",
    "Polynomial Features: Create higher-order terms (e.g., square or cubic) of continuous features.\n",
    "Benefits:\n",
    "Introduces non-linearity.\n",
    "Captures complex interactions.\n",
    "Enhances model expressiveness.\n",
    "Example: In a recommendation system, create interaction features between user preferences (genres, ratings) and item features (actors, directors) to improve personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1') Question 8:__ Describe a real-life NLP related application that is achievable using skills you learned in this lab? Note, please make sure it's siginficantly different than what is covered in class. <br>\n",
    "__Ans:__ Application: Personalized Recipe Recommendation System\n",
    "\n",
    "Imagine a personalized recipe recommendation system that leverages NLP techniques to enhance the culinary experience for users. Here’s how it works:\n",
    "\n",
    "User Interaction and Data Collection:\n",
    "Users interact with the system through a mobile app or website.\n",
    "They provide information about their dietary preferences, allergies, cooking skills, and flavor preferences.\n",
    "The system collects data on their past recipe choices, ingredient preferences, and cooking habits.\n",
    "Text Analysis and Ingredient Extraction:\n",
    "When a user searches for recipes or inputs a description (e.g., “I want a hearty vegetarian pasta”), the system performs text analysis.\n",
    "It extracts relevant keywords (e.g., “vegetarian,” “pasta,” “hearty”) and identifies the user’s intent.\n",
    "Recipe Corpus and Semantic Search:\n",
    "The system maintains a large corpus of recipes from various cuisines.\n",
    "Using semantic search techniques (such as word embeddings or BERT), it matches the user’s intent with relevant recipes.\n",
    "For instance, if the user mentions “comfort food,” the system retrieves recipes for macaroni and cheese, lasagna, or creamy soups.\n",
    "Personalization and Adaptation:\n",
    "The system considers the user’s dietary restrictions (e.g., gluten-free, vegan) and adjusts the search results accordingly.\n",
    "It also takes into account the user’s cooking skills (beginner, intermediate, advanced) and suggests recipes that match their proficiency level.\n",
    "Ingredient Substitution and Customization:\n",
    "NLP models analyze recipe ingredients and instructions.\n",
    "If a user dislikes a specific ingredient (e.g., mushrooms), the system suggests suitable substitutions (e.g., spinach or zucchini).\n",
    "It adapts recipes based on available ingredients at home (e.g., “I have chicken, tomatoes, and basil; what can I make?”).\n",
    "Sentiment Analysis and User Feedback:\n",
    "After trying a recipe, users provide feedback.\n",
    "Sentiment analysis determines whether they enjoyed the dish, found it easy to follow, or encountered any issues.\n",
    "The system learns from this feedback to improve future recommendations.\n",
    "Dynamic Recipe Generation:\n",
    "For creative users, the system generates new recipes by combining existing ones.\n",
    "It uses language models (e.g., GPT) to create unique recipe descriptions and instructions.\n",
    "Users can explore innovative dishes like “Thai-inspired avocado tacos” or “chocolate-infused risotto.”\n",
    "Multilingual Support and Cultural Adaptation:\n",
    "The system supports multiple languages and adapts to regional cuisines.\n",
    "If a user switches from English to Spanish, it seamlessly provides recipes for paella or churros.\n",
    "Cooking Tips and Techniques:\n",
    "NLP-powered chatbots assist users during cooking.\n",
    "They answer questions like “How do I julienne carrots?” or “What’s the best way to sear a steak?”\n",
    "Community Interaction and Social Features:\n",
    "Users can share their culinary creations, modifications, and food photography.\n",
    "The system encourages a community of food enthusiasts who exchange tips and tricks.\n",
    "In summary, this personalized recipe recommendation system combines NLP, semantic search, sentiment analysis, and creativity to revolutionize home cooking. It caters to individual tastes, dietary needs, and cooking abilities, making mealtime delightful and adventurous! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
